{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75cbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  learning_rate, n_estimator 영향 제일 많이 끼치는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32915494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb8e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 17:20:42.056648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier # decision tree 앙상블 모델, 부스팅\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# 모델 평가 지표 scoring metrics\n",
    "from sklearn.model_selection import cross_val_score # model 검증\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import ast # convert string to function\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0141df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 함수\n",
    "def mk_cd(x):\n",
    "    if type(x) == str:\n",
    "        if x.isalpha():\n",
    "            return x\n",
    "        else:\n",
    "            if len(x) == 6:\n",
    "                return x\n",
    "            else:\n",
    "                return \"0\"*(6-len(x))+x\n",
    "    elif type(x) == int:\n",
    "        x = str(x)\n",
    "        return \"0\"*(6-len(x))+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b118c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_a2012 = 107935\n",
    "len_a2013 = 93840\n",
    "len_a2014 = 95437\n",
    "len_a2015 = 158287\n",
    "len_a2016 = 134332\n",
    "len_a2017 = 110782\n",
    "len_a2018 = 159382\n",
    "len_a2019 = 133987\n",
    "len_a2020 = 222267\n",
    "len_a2021 = 223622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61f6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len1215 = len_a2012+len_a2013+len_a2014+len_a2015 # y:2015\n",
    "len1316 = len1215+len_a2016 # y:2016\n",
    "len1417 = len1316+len_a2017 # y:2017\n",
    "len1518 = len1417+len_a2018 # y:2018\n",
    "len1619 = len1518+len_a2019 # y:2019\n",
    "len1720 = len1619+len_a2020 # y:2020\n",
    "len1821 = len1720+len_a2021 # y:2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac7995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_dic = {'1215':[0, len1215, len_a2015],\n",
    "               '1316':[len_a2012, len1316, len_a2016],\n",
    "               '1417':[len_a2012+len_a2013, len1417, len_a2017],\n",
    "               '1518':[len_a2012+len_a2013+len_a2014, len1518, len_a2018],\n",
    "               '1619':[len_a2012+len_a2013+len_a2014+len_a2015, len1619, len_a2019],\n",
    "               '1720':[len_a2012+len_a2013+len_a2014+len_a2015+len_a2016,len1720, len_a2020],\n",
    "               '1821':[len_a2012+len_a2013+len_a2014+len_a2015+len_a2016+len_a2017,len1821, len_a2021]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2670ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1221 = pd.read_feather('../kelly/d1221_10thr.ftr', columns = None, use_threads = True)\n",
    "d1221['code'] = list(map(mk_cd, d1221['code'])) # mk_cd함수 적용시켜야 ftr만들 때 에러 안남.\n",
    "# NextChange\n",
    "d1221['NextChange'] = np.where(d1221['NextChange']>0.05, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c60f59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree = DecisionTreeClassifier()\n",
    "DecisionTree.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cd2fba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1396494/449322708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m clf = RandomizedSearchCV(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m#파이프라인으로 학습된 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#하이퍼파라미터 튜닝 값으로 진행한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# 반복 횟수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "## RandomizedSearch의 각 하이퍼파라미터의 설명\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    pipe,                       #파이프라인으로 학습된 모델 \n",
    "    param_distributions=dists,  #하이퍼파라미터 튜닝 값으로 진행한다\n",
    "    n_iter=50,                  # 반복 횟수\n",
    "    cv=3,                       # 교차검증 횟수 (==> n_iter * cv 의 숫자만큼 진행됨)\n",
    "    scoring='neg_mean_absolute_error', # MAE\n",
    "    verbose=1,                  # 훈련 중지여부를 화면에 출력 (1= progress bar/2= one line per epoch)\n",
    "    n_jobs=-1)                  # -1: 모든 CPU 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1ceaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    785677\n",
       "1    654194\n",
       "Name: NextChange, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "y.value_counts()\n",
    "785633/(785633+785633)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63077244",
   "metadata": {},
   "source": [
    "# GaussianNB 하이퍼 파라미터 찾기. (이거 참고해)\n",
    "var smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e73f0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "17:21:12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00     85757\n",
      "           1       0.46      1.00      0.63     72530\n",
      "\n",
      "    accuracy                           0.46    158287\n",
      "   macro avg       0.48      0.50      0.31    158287\n",
      "weighted avg       0.48      0.46      0.29    158287\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.7000000000000001)\n",
      "---------0번째 search---------:  0.45821829967085104\n",
      "<<<---1316 start--->>>\n",
      "17:22:20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347564"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "134332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.00      0.00     73208\n",
      "           1       0.46      1.00      0.63     61124\n",
      "\n",
      "    accuracy                           0.46    134332\n",
      "   macro avg       0.55      0.50      0.31    134332\n",
      "weighted avg       0.55      0.46      0.29    134332\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.2)\n",
      "---------0번째 search---------:  0.45513354971265224\n",
      "<<<---1417 start--->>>\n",
      "17:23:37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388056"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.00      0.00     61330\n",
      "           1       0.45      1.00      0.62     49452\n",
      "\n",
      "    accuracy                           0.45    110782\n",
      "   macro avg       0.51      0.50      0.31    110782\n",
      "weighted avg       0.51      0.45      0.28    110782\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.2)\n",
      "---------0번째 search---------:  0.4465707425394017\n",
      "<<<---1518 start--->>>\n",
      "17:25:26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403401"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "159382"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.00      0.01     87547\n",
      "           1       0.45      1.00      0.62     71835\n",
      "\n",
      "    accuracy                           0.45    159382\n",
      "   macro avg       0.52      0.50      0.31    159382\n",
      "weighted avg       0.53      0.45      0.28    159382\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.9)\n",
      "---------0번째 search---------:  0.4512115546297574\n",
      "<<<---1619 start--->>>\n",
      "17:27:19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404496"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "133987"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.00      0.00     74001\n",
      "           1       0.45      1.00      0.62     59986\n",
      "\n",
      "    accuracy                           0.45    133987\n",
      "   macro avg       0.52      0.50      0.31    133987\n",
      "weighted avg       0.53      0.45      0.28    133987\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.1)\n",
      "---------0번째 search---------:  0.4478941986909178\n",
      "<<<---1720 start--->>>\n",
      "17:29:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404151"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222267"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.00      0.00    118424\n",
      "           1       0.47      1.00      0.64    103843\n",
      "\n",
      "    accuracy                           0.47    222267\n",
      "   macro avg       0.53      0.50      0.32    222267\n",
      "weighted avg       0.53      0.47      0.30    222267\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.4)\n",
      "---------0번째 search---------:  0.46749180040221894\n",
      "<<<---1821 start--->>>\n",
      "17:30:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "515636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "223622"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=GaussianNB(),\n",
       "                   param_distributions={'var_smoothing': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.00      0.00    123556\n",
      "           1       0.45      1.00      0.62    100066\n",
      "\n",
      "    accuracy                           0.45    223622\n",
      "   macro avg       0.53      0.50      0.31    223622\n",
      "weighted avg       0.54      0.45      0.28    223622\n",
      "\n",
      "best : GaussianNB(var_smoothing=0.30000000000000004)\n",
      "---------0번째 search---------:  0.44778689037751207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "670.5186607837677"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = GaussianNB() ########여기에 모델이름!!!!함수로 적을것###########\n",
    "random_search = {'var_smoothing': np.linspace(0.1, 1, 10)} ###### 여기에 하이퍼 파라미터랑 넣을 값 적으면 됨!!!#######\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "    print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(f'---------{i}번째 search---------: ',acc3)\n",
    "    i+=1\n",
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7096290",
   "metadata": {},
   "outputs": [],
   "source": [
    "gau_list = [[' GaussianNB(var_smoothing=0.7000000000000001)',0.45821829967085104],\n",
    "           ['GaussianNB(var_smoothing=0.2)',0.45513354971265224],\n",
    "           ['GaussianNB(var_smoothing=0.2)', 0.4465707425394017],\n",
    "           ['GaussianNB(var_smoothing=0.9)',0.4512115546297574],\n",
    "           ['GaussianNB(var_smoothing=0.1)', 0.4478941986909178],\n",
    "           ['GaussianNB(var_smoothing=0.4)',0.46749180040221894],\n",
    "           ['GaussianNB(var_smoothing=0.30000000000000004)',0.44778689037751207]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e30b61",
   "metadata": {},
   "source": [
    "# logistic regression 파라미터\n",
    "max iter, random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02a0836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "17:33:49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=398.9795918367347, random_state=100)\n",
      " acc:   0.5416932533941511\n",
      "<<<---1316 start--->>>\n",
      "17:44:13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347564"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "134332"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=196.9387755102041, random_state=100)\n",
      " acc:   0.5448664502873478\n",
      "<<<---1417 start--->>>\n",
      "17:55:20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388056"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110782"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=141.83673469387756, random_state=100)\n",
      " acc:   0.5534021772490115\n",
      "<<<---1518 start--->>>\n",
      "18:08:32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403401"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "159382"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=141.83673469387756, random_state=100)\n",
      " acc:   0.5483492489741627\n",
      "<<<---1619 start--->>>\n",
      "18:22:40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "133987"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=141.83673469387756, random_state=100)\n",
      " acc:   0.5505981923619455\n",
      "<<<---1720 start--->>>\n",
      "18:36:35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404151"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222267"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=141.83673469387756, random_state=100)\n",
      " acc:   0.5323057403933108\n",
      "<<<---1821 start--->>>\n",
      "18:50:27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "515636"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "223622"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "                   param_distributions={'max_iter': array([ 50.        ,  59.18367347,  68.36734694,  77.55102041,\n",
       "        86.73469388,  95.91836735, 105.10204082, 114.28571429,\n",
       "       123.46938776, 132.65306122, 141.83673469, 151.02040816,\n",
       "       160.20408163, 169.3877551 , 178.57142857, 187.75510204,\n",
       "       196.93877551, 206.12244898, 215.30612245, 224.48979592,\n",
       "       233.6734693...\n",
       "       270.40816327, 279.59183673, 288.7755102 , 297.95918367,\n",
       "       307.14285714, 316.32653061, 325.51020408, 334.69387755,\n",
       "       343.87755102, 353.06122449, 362.24489796, 371.42857143,\n",
       "       380.6122449 , 389.79591837, 398.97959184, 408.16326531,\n",
       "       417.34693878, 426.53061224, 435.71428571, 444.89795918,\n",
       "       454.08163265, 463.26530612, 472.44897959, 481.63265306,\n",
       "       490.81632653, 500.        ]),\n",
       "                                        'random_state': [0, 100]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : LogisticRegression(max_iter=141.83673469387756, random_state=100)\n",
      " acc:   0.5518061729167971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5781.384303569794"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2=[]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = LogisticRegression()\n",
    "random_search = {'max_iter': np.linspace(50, 500, 50), \n",
    "                 'random_state': [0, 100]}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "#     print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(' acc:  ',acc3)\n",
    "    l2.append([i,model.best_estimator_, acc3])\n",
    "    i+=1\n",
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009cbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_best\n",
    "log_list=[['LogisticRegression(max_iter=398.9795918367347, random_state=100)',0.5416932533941511],\n",
    "          ['LogisticRegression(max_iter=196.9387755102041, random_state=100)',0.5448664502873478],\n",
    "          ['LogisticRegression(max_iter=141.83673469387756, random_state=100)',0.5534021772490115],\n",
    "          ['LogisticRegression(max_iter=141.83673469387756, random_state=100)', 0.5483492489741627],\n",
    "          ['LogisticRegression(max_iter=141.83673469387756, random_state=100)',0.5505981923619455],\n",
    "          ['LogisticRegression(max_iter=141.83673469387756, random_state=100)',0.5323057403933108],\n",
    "          ['LogisticRegression(max_iter=141.83673469387756, random_state=100)',0.5518061729167971]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274416f4",
   "metadata": {},
   "source": [
    "# Random Forest 하이퍼파라미터 찾기\n",
    "max leaf nodes\n",
    "- decision Tree, Gradient Boosting과 공유할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ea38c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "19:12:41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5461787765261834\n",
      "<<<---1316 start--->>>\n",
      "19:53:11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347564"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "134332"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.553233778995325\n",
      "<<<---1417 start--->>>\n",
      "20:38:13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388056"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110782"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5563629470491596\n",
      "<<<---1518 start--->>>\n",
      "21:28:07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "159382"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5563802687881944\n",
      "<<<---1619 start--->>>\n",
      "22:20:31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404496"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "133987"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5555389701986013\n",
      "<<<---1720 start--->>>\n",
      "23:11:40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404151"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5312484534366325\n",
      "<<<---1821 start--->>>\n",
      "00:00:55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "515636"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "223622"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_leaf_nodes': [4, 5, 6, 7, 8]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : RandomForestClassifier(max_leaf_nodes=8)\n",
      " acc:   0.5553746947974707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21589.794280052185"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[0, RandomForestClassifier(max_leaf_nodes=8), 0.5461787765261834],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.553233778995325],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5563629470491596],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5563802687881944],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5555389701986013],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5312484534366325],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5553746947974707]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_list=[]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "# user_seed = 0\n",
    "# random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "random_search = {'max_leaf_nodes': [4,5,6,7,8]} ## clf에서 사용되고 있던 파라미터 가져와봄\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "#     print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(' acc:  ',acc3)\n",
    "    ran_list.append([i,model.best_estimator_,acc3])\n",
    "    i+=1\n",
    "time.time()-start\n",
    "ran_list\n",
    "# 23:00:16 start\n",
    "# 01:48:-- 미완\n",
    "# randomforest max_leaf_nodes 5가지로 조정 16:48:28 -> 텀 47분~52분 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cacd1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, RandomForestClassifier(max_leaf_nodes=8), 0.5461787765261834],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.553233778995325],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5563629470491596],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5563802687881944],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5555389701986013],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5312484534366325],\n",
       " [0, RandomForestClassifier(max_leaf_nodes=8), 0.5553746947974707]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62d297",
   "metadata": {},
   "source": [
    "# XGBoost 하이퍼파라미터 찾기\n",
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392d04e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "01:12:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[01:12:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:14:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:16:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:18:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:20:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:22:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:23:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:25:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:27:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:29:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:32:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:34:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5584160417469534\n",
      "<<<---1316 start--->>>\n",
      "01:36:51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347564"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "134332"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[01:36:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:41:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:43:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:46:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:48:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:50:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:52:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:55:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:58:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:01:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:03:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:05:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5633207277491588\n",
      "<<<---1417 start--->>>\n",
      "02:07:17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388056"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110782"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[02:07:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:09:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:11:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:13:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:15:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:17:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:19:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:21:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:23:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:25:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:27:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:30:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:32:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5632142405805998\n",
      "<<<---1518 start--->>>\n",
      "02:34:18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403401"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "159382"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[02:34:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:36:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:40:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:44:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:46:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:48:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:50:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:52:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:54:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:56:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:58:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5518439974401125\n",
      "<<<---1619 start--->>>\n",
      "03:00:22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404496"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "133987"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[03:00:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:02:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:04:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:06:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:08:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:10:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:12:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:14:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:16:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:18:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:20:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:22:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:24:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5583078955421048\n",
      "<<<---1720 start--->>>\n",
      "03:26:13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404151"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222267"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[03:26:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:30:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:32:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:34:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:36:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:40:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:44:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:46:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:50:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:52:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.5332865427616336\n",
      "<<<---1821 start--->>>\n",
      "03:54:29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "515636"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "223622"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[03:54:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:56:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:58:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:01:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:04:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:06:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:09:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:11:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:13:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:17:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:20:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:25:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:28:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   param_distributions={'learning_rate': [0.001, 0.005, 0.05,\n",
       "                                                          0.2]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      " acc:   0.553143250664067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11891.889689922333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1=[]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = XGBClassifier()\n",
    "random_search = {'learning_rate': [0.001, 0.005, 0.05,0.2]}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "#     print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(' acc:  ',acc3)\n",
    "    l1.append([i, model.best_estimator_, acc3])\n",
    "    i+=1\n",
    "time.time()-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce33641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5584160417469534],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5633207277491588],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5632142405805998],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5518439974401125],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5583078955421048],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.5332865427616336],\n",
       " [0,\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "                gamma=0, gpu_id=-1, importance_type=None,\n",
       "                interaction_constraints='', learning_rate=0.2, max_delta_step=0,\n",
       "                max_depth=6, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints='()', n_estimators=100, n_jobs=128,\n",
       "                num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "                reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "                tree_method='exact', validate_parameters=1, verbosity=None),\n",
       "  0.553143250664067]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10d40a",
   "metadata": {},
   "source": [
    "# MLP 하이퍼파라미터 찾기\n",
    "learning rate init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f8730b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "04:30:42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier()\n",
      " acc:   0.5410109484670251\n",
      "<<<---1316 start--->>>\n",
      "05:00:06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347564"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "134332"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier(learning_rate_init=0.005)\n",
      " acc:   0.5444272399726052\n",
      "<<<---1417 start--->>>\n",
      "05:22:11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388056"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110782"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier()\n",
      " acc:   0.5533841237746204\n",
      "<<<---1518 start--->>>\n",
      "05:48:06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403401"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "159382"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier()\n",
      " acc:   0.5490205920367419\n",
      "<<<---1619 start--->>>\n",
      "06:11:59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404496"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "133987"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier()\n",
      " acc:   0.5522476061110406\n",
      "<<<---1720 start--->>>\n",
      "06:37:13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404151"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222267"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier(learning_rate_init=0.05)\n",
      " acc:   0.5321977621509266\n",
      "<<<---1821 start--->>>\n",
      "07:02:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "515636"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "223622"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "                   param_distributions={'learning_rate_init': [0.005, 0.05,\n",
       "                                                               0.0005, 0.001]},\n",
       "                   random_state=100, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best : MLPClassifier(learning_rate_init=0.05)\n",
      " acc:   0.5520029335217466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11107.739107608795"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[0, MLPClassifier(), 0.5410109484670251],\n",
       " [0, MLPClassifier(learning_rate_init=0.005), 0.5444272399726052],\n",
       " [0, MLPClassifier(), 0.5533841237746204],\n",
       " [0, MLPClassifier(), 0.5490205920367419],\n",
       " [0, MLPClassifier(), 0.5522476061110406],\n",
       " [0, MLPClassifier(learning_rate_init=0.05), 0.5321977621509266],\n",
       " [0, MLPClassifier(learning_rate_init=0.05), 0.5520029335217466]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2=[]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = MLPClassifier()\n",
    "random_search = {'learning_rate_init': [0.005, 0.05, 0.0005, 0.001 ]}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "#     print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(' acc:  ',acc3)\n",
    "    l2.append([i, model.best_estimator_, acc3])\n",
    "    i+=1\n",
    "time.time()-start\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87670f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0c6ded",
   "metadata": {},
   "source": [
    "# Support Vector Machine 하이퍼 파라미터\n",
    "C , degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f317219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<---1215 start--->>>\n",
      "07:35:50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "158287"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "svc_list=[]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "start = time.time()\n",
    "# X, y\n",
    "X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "y = d1221.iloc[:,-1]\n",
    "\n",
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "\n",
    "clf = SVC()\n",
    "random_search = {'C': [0.2, 1.0, 10.0],\n",
    "                'gamma': [0.5, 1, 3]}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(estimator = clf, \n",
    "                           param_distributions = random_search,\n",
    "                           cv =3,\n",
    "                           verbose = 1,\n",
    "                           random_state = 100)\n",
    "\n",
    "for slicer in slicing_dic:\n",
    "    i=0\n",
    "    print(f'<<<---{slicer} start--->>>')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))\n",
    "    tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "    tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "    X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "    y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "    \n",
    "    # train끼리 idx 같아야 하므로\n",
    "    len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "    len(list(range(len_train)))\n",
    "    train_idx = list(range(len_train))\n",
    "    random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    # test끼리 idx 같아야 하므로\n",
    "    len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "    test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "    len(test_idx)\n",
    "    random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "    \n",
    "    X_train = X_train.loc[train_idx]\n",
    "    y_train = y_train.loc[train_idx]\n",
    "    X_test = X_test.loc[test_idx]\n",
    "    y_test = y_test.loc[test_idx]\n",
    "    ###########################################\n",
    "    y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    predictiontree = model.best_estimator_.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,predictiontree))\n",
    "#     print(classification_report(y_test,predictiontree))\n",
    "    print('best :', model.best_estimator_)\n",
    "    acc3=accuracy_score(y_test,predictiontree)\n",
    "    print(' acc:  ',acc3)\n",
    "    svc_list.append([i, model.best_estimator_, acc3])\n",
    "    i+=1\n",
    "time.time()-start\n",
    "svc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ac783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
